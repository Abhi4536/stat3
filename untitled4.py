# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CEsb4VW1V56rUSUQneA3DXhz1MQPIavt
"""



"""1. What is hypothesis testing in statistics?
Hypothesis testing is a statistical method used to make decisions or inferences about population parameters based on sample data. It involves testing an assumption (hypothesis) about a population using probability theory.

2. What is the null hypothesis, and how does it differ from the alternative hypothesis?
Null Hypothesis (H₀): Assumes no effect or no difference. It's the default assumption.

Alternative Hypothesis (H₁ or Ha): Contradicts the null hypothesis. It suggests a significant effect or difference exists.

3. What is the significance level in hypothesis testing, and why is it important?
The significance level (α) is the probability of rejecting the null hypothesis when it is actually true. Common values are 0.05, 0.01.

It determines the threshold for statistical significance and controls the risk of making a Type 1 error.

4. What does a P-value represent in hypothesis testing?
The P-value is the probability of obtaining test results at least as extreme as the observed results, assuming the null hypothesis is true.

5. How do you interpret the P-value in hypothesis testing?
If P ≤ α: Reject the null hypothesis (evidence supports the alternative).

If P > α: Fail to reject the null hypothesis (insufficient evidence to support the alternative).

6. What are Type 1 and Type 2 errors in hypothesis testing?
Type 1 Error (α): Rejecting a true null hypothesis (false positive).

Type 2 Error (β): Failing to reject a false null hypothesis (false negative).

7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?
One-tailed test: Tests for an effect in one direction (e.g., greater than or less than).

Two-tailed test: Tests for an effect in both directions (e.g., not equal to).

8. What is the Z-test, and when is it used in hypothesis testing?
A Z-test is used when the population standard deviation is known and the sample size is large (n > 30). It tests whether the sample mean differs from a known population mean.

9. How do you calculate the Z-score, and what does it represent in hypothesis testing?
Formula:

𝑍
=
𝑥
ˉ
−
𝜇
𝜎
/
𝑛
Z=
σ/
n
​

x
ˉ
 −μ
​

Where:

𝑥
ˉ
x
ˉ
  = sample mean

𝜇
μ = population mean

𝜎
σ = population standard deviation

𝑛
n = sample size

Interpretation: Tells how many standard deviations the sample mean is from the population mean.

10. What is the T-distribution, and when should it be used instead of the normal distribution?
T-distribution is used when the sample size is small (n < 30) and/or the population standard deviation is unknown.

It's wider and has heavier tails than the normal distribution.

11. What is the difference between a Z-test and a T-test?
Z-test: Known population standard deviation, large sample.

T-test: Unknown population standard deviation, small sample.

12. What is the T-test, and how is it used in hypothesis testing?
A T-test compares sample means to determine if there's a significant difference:

One-sample t-test: Sample vs. population

Two-sample t-test: Two independent samples

Paired t-test: Same group at two time points

13. What is the relationship between Z-test and T-test in hypothesis testing?
Both test hypotheses about means. The T-test is used when variability is estimated from the sample, while the Z-test uses known population variance. As sample size increases, the T-distribution approaches the normal distribution, making both tests similar.

14. What is a confidence interval, and how is it used to interpret statistical results?
A confidence interval (CI) gives a range of values likely to contain the population parameter.

Example: A 95% CI means there's a 95% chance the true value lies within that interval.

15. What is the margin of error, and how does it affect the confidence interval?
The margin of error indicates the range of uncertainty around the estimate.

Larger margin = wider confidence interval = more uncertainty.

16. How is Bayes' Theorem used in statistics, and what is its significance?
Bayes’ Theorem updates the probability of a hypothesis based on new evidence.

𝑃
(
𝐴
∣
𝐵
)
=
𝑃
(
𝐵
∣
𝐴
)
⋅
𝑃
(
𝐴
)
𝑃
(
𝐵
)
P(A∣B)=
P(B)
P(B∣A)⋅P(A)
​

It’s key in Bayesian statistics, decision-making, and machine learning.

17. What is the Chi-square distribution, and when is it used?
The Chi-square distribution is used for:

Testing independence between categorical variables

Checking goodness of fit

Testing variance

18. What is the Chi-square goodness of fit test, and how is it applied?
It tests whether observed frequencies match expected frequencies in categorical data.
Formula:

𝜒
2
=
∑
(
𝑂
−
𝐸
)
2
𝐸
χ
2
 =∑
E
(O−E)
2

​

Where O = observed, E = expected.

19. What is the F-distribution, and when is it used in hypothesis testing?
The F-distribution is used to compare two variances or in ANOVA.

It’s right-skewed and based on the ratio of variances.

20. What is an ANOVA test, and what are its assumptions?
ANOVA (Analysis of Variance) tests if there are significant differences between the means of 3 or more groups.

Assumptions:

Independence

Normality

Equal variances (homogeneity)

21. What are the different types of ANOVA tests?
One-way ANOVA: One independent variable.

Two-way ANOVA: Two independent variables.

Repeated measures ANOVA: Same subjects in all conditions.

22. What is the F-test, and how does it relate to hypothesis testing?
An F-test compares two variances or is used in ANOVA to test the overall model significance.

It's a ratio of two variances:

𝐹
=
variance between groups
variance within groups
F=
variance within groups
variance between groups
​


"""

#Write a Python program to generate a random variable and display its value
import numpy as np

random_value = np.random.rand()
print("Random Value:", random_value)

# Generate a discrete uniform distribution using Python and plot the probability mass function (PMF)
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import randint

x = np.arange(1, 7)
pmf = randint.pmf(x, 1, 7)

plt.stem(x, pmf, use_line_collection=True)
plt.title("PMF of Discrete Uniform Distribution (Die Roll)")
plt.xlabel("Value")
plt.ylabel("Probability")
plt.grid(True)
plt.show()

#Write a Python function to calculate the probability distribution function (PDF) of a Bernoulli distribution
from scipy.stats import bernoulli

def bernoulli_pdf(p, x):
    return bernoulli.pmf(x, p)

print("P(X=1) with p=0.6:", bernoulli_pdf(0.6, 1))
print("P(X=0) with p=0.6:", bernoulli_pdf(0.6, 0))

#Write a Python script to simulate a binomial distribution with n=10 and p=0.5, then plot its histogram
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

n, p = 10, 0.5
data = np.random.binomial(n, p, 1000)

plt.hist(data, bins=np.arange(n+2)-0.5, density=True, edgecolor='black')
plt.title("Binomial Distribution (n=10, p=0.5)")
plt.xlabel("Number of Successes")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

#Create a Poisson distribution and visualize it using Python
from scipy.stats import poisson
import matplotlib.pyplot as plt

mu = 3
x = np.arange(0, 10)
pmf = poisson.pmf(x, mu)

plt.stem(x, pmf, use_line_collection=True)
plt.title("Poisson Distribution (μ = 3)")
plt.xlabel("x")
plt.ylabel("P(X=x)")
plt.grid(True)
plt.show()

#Write a Python program to calculate and plot the cumulative distribution function (CDF) of a discret
from scipy.stats import randint

x = np.arange(1, 7)
cdf = randint.cdf(x, 1, 7)

plt.step(x, cdf, where="mid")
plt.title("CDF of Discrete Uniform Distribution (Die Roll)")
plt.xlabel("x")
plt.ylabel("Cumulative Probability")
plt.grid(True)
plt.show()

#Generate a continuous uniform distribution using NumPy and visualize it
from scipy.stats import uniform

data = uniform.rvs(loc=0, scale=1, size=1000)

plt.hist(data, bins=30, density=True, edgecolor='black')
plt.title("Continuous Uniform Distribution")
plt.xlabel("Value")
plt.ylabel("Density")
plt.grid(True)
plt.show()

#Simulate data from a normal distribution and plot its histogram
data = np.random.normal(loc=0, scale=1, size=1000)

plt.hist(data, bins=30, density=True, edgecolor='black')
plt.title("Histogram of Normally Distributed Data")
plt.xlabel("Value")
plt.ylabel("Density")
plt.grid(True)
plt.show()

#Write a Python function to calculate Z-scores from a dataset and plot them
from scipy.stats import zscore

data = np.random.normal(50, 10, 1000)
z_scores = zscore(data)

plt.hist(z_scores, bins=30, edgecolor='black')
plt.title("Z-scores of Data")
plt.xlabel("Z-score")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# Implement the Central Limit Theorem (CLT) using Python for a non-normal distribution
# CLT with Exponential Distribution
samples = 1000
sample_size = 30
means = []

for _ in range(samples):
    sample = np.random.exponential(scale=2.0, size=sample_size)
    means.append(np.mean(sample))

plt.hist(means, bins=30, edgecolor='black', density=True)
plt.title("CLT Demonstration with Exponential Distribution")
plt.xlabel("Sample Mean")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

#Simulate multiple samples from a normal distribution and verify the Central Limit Theorem
import numpy as np
import matplotlib.pyplot as plt

means = []
for _ in range(1000):  # 1000 samples
    sample = np.random.normal(loc=50, scale=10, size=30)  # mean=50, std=10
    means.append(np.mean(sample))

plt.hist(means, bins=30, edgecolor='black', density=True)
plt.title("Central Limit Theorem (Sampling Distribution of Mean)")
plt.xlabel("Sample Mean")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

#Write a Python function to calculate and plot the standard normal distribution (mean = 0, std = 1)
from scipy.stats import norm

def plot_standard_normal():
    x = np.linspace(-4, 4, 1000)
    y = norm.pdf(x, loc=0, scale=1)

    plt.plot(x, y, label='Standard Normal PDF')
    plt.title("Standard Normal Distribution")
    plt.xlabel("Z")
    plt.ylabel("Density")
    plt.grid(True)
    plt.legend()
    plt.show()

plot_standard_normal()

#Generate random variables and calculate their corresponding probabilities using the binomial distribution
from scipy.stats import binom

n, p = 10, 0.5
x = np.arange(0, n+1)
probs = binom.pmf(x, n, p)

for i in x:
    print(f"P(X={i}) = {binom.pmf(i, n, p):.3f}")

#Write a Python program to calculate the Z-score for a given data point and compare it to a standard normal
def z_score(x, mean, std):
    return (x - mean) / std

value = 75
mean = 70
std_dev = 10
z = z_score(value, mean, std_dev)
print(f"Z-score of {value} is {z:.2f}")

#C Implement hypothesis testing using Z-statistics for a sample dataset
from scipy.stats import norm

# Sample data
sample_mean = 102
population_mean = 100
std_dev = 5
n = 30

z = (sample_mean - population_mean) / (std_dev / np.sqrt(n))
p_value = 1 - norm.cdf(z)

print("Z-statistic:", round(z, 2))
print("P-value:", round(p_value, 4))

if p_value < 0.05:
    print("Reject the null hypothesis.")
else:
    print("Fail to reject the null hypothesis.")

#Create a confidence interval for a dataset using Python and interpret the result
import scipy.stats as stats

data = np.random.normal(50, 10, 100)
mean = np.mean(data)
sem = stats.sem(data)
confidence = 0.95

ci = stats.t.interval(confidence, len(data)-1, loc=mean, scale=sem)
print(f"95% Confidence Interval: {ci}")

#Generate data from a normal distribution, then calculate and interpret the confidence interval for its mean
mean = np.mean(data)
std = np.std(data)
ci = stats.norm.interval(0.95, loc=mean, scale=std/np.sqrt(len(data)))

plt.hist(data, bins=30, alpha=0.7, edgecolor='black')
plt.axvline(ci[0], color='red', linestyle='--', label="Lower CI")
plt.axvline(ci[1], color='green', linestyle='--', label="Upper CI")
plt.title("Confidence Interval on Normal Data")
plt.legend()
plt.show()

# Write a Python script to calculate and visualize the probability density function (PDF) of a normal distribution
x = np.linspace(-4, 4, 1000)
pdf = norm.pdf(x)

plt.plot(x, pdf)
plt.title("PDF of Normal Distribution")
plt.xlabel("x")
plt.ylabel("Probability Density")
plt.grid(True)
plt.show()

#Use Python to calculate and interpret the cumulative distribution function (CDF) of a Poisson distribution
from scipy.stats import poisson

x = np.arange(0, 15)
mu = 5
cdf = poisson.cdf(x, mu)

plt.step(x, cdf, where='mid')
plt.title("CDF of Poisson Distribution (μ = 5)")
plt.xlabel("x")
plt.ylabel("Cumulative Probability")
plt.grid(True)
plt.show()

#Simulate a random variable using a continuous uniform distribution and calculate its expected valu
data = np.random.uniform(10, 20, 1000)
expected_value = np.mean(data)
print("Expected Value:", expected_value)

#Write a Python program to compare the standard deviations of two datasets and visualize the difference
data1 = np.random.normal(50, 5, 100)
data2 = np.random.normal(50, 15, 100)

std1, std2 = np.std(data1), np.std(data2)

plt.hist(data1, alpha=0.5, label='Dataset 1', bins=30)
plt.hist(data2, alpha=0.5, label='Dataset 2', bins=30)
plt.legend()
plt.title(f"STD Comparison | STD1: {std1:.2f}, STD2: {std2:.2f}")
plt.show()

#C Calculate the range and interquartile range (IQR) of a dataset generated from a normal distribution
data = np.random.normal(0, 1, 1000)
range_val = np.max(data) - np.min(data)
iqr_val = np.percentile(data, 75) - np.percentile(data, 25)

print(f"Range: {range_val:.2f}")
print(f"IQR: {iqr_val:.2f}")

# Implement Z-score normalization on a dataset and visualize its transformation
z_scores = zscore(data)

plt.hist(data, alpha=0.5, label="Original")
plt.hist(z_scores, alpha=0.5, label="Z-score Normalized")
plt.legend()
plt.title("Z-score Normalization")
plt.grid(True)
plt.show()

#Write a Python function to calculate the skewness and kurtosis of a dataset generated from a normal
from scipy.stats import skew, kurtosis

data = np.random.normal(loc=0, scale=1, size=1000)
print("Skewness:", skew(data))
print("Kurtosis:", kurtosis(data))

#Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and
#interpret the results
from scipy.stats import norm
import numpy as np

sample = np.array([105, 110, 98, 115, 108])
pop_mean = 100
pop_std = 10
n = len(sample)

sample_mean = np.mean(sample)
z = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))
p_value = 2 * (1 - norm.cdf(abs(z)))

print("Z-statistic:", z)
print("P-value:", p_value)

if p_value < 0.05:
    print("Reject the null hypothesis")
else:
    print("Fail to reject the null hypothesis")

#Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python
sample = np.random.normal(102, 8, 40)
pop_mean = 100
pop_std = 10
z = (np.mean(sample) - pop_mean) / (pop_std / np.sqrt(len(sample)))
p = 2 * (1 - norm.cdf(abs(z)))

print(f"Z = {z:.2f}, P-value = {p:.4f}")

#mplement a one-sample Z-test using Python to compare the sample mean with the population mean
def one_sample_z_test(sample, pop_mean, pop_std):
    n = len(sample)
    z = (np.mean(sample) - pop_mean) / (pop_std / np.sqrt(n))
    p = 2 * (1 - norm.cdf(abs(z)))
    return z, p

sample = np.random.normal(103, 10, 30)
z, p = one_sample_z_test(sample, 100, 10)
print("Z-score:", z, "| P-value:", p)

#Perform a two-tailed Z-test using Python and visualize the decision region on a plot
import matplotlib.pyplot as plt

z_score = 2.1
x = np.linspace(-4, 4, 1000)
y = norm.pdf(x)

plt.plot(x, y)
plt.fill_between(x, y, where=(x < -1.96) | (x > 1.96), color='red', alpha=0.3, label="Rejection Region")
plt.axvline(z_score, color='blue', linestyle='--', label=f"Z = {z_score:.2f}")
plt.title("Two-Tailed Z-test")
plt.legend()
plt.grid(True)
plt.show()

#Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing
def visualize_type_errors(mu0=100, mu1=105, sigma=10, n=30, alpha=0.05):
    x = np.linspace(80, 120, 1000)
    se = sigma / np.sqrt(n)

    # Null and alternative distributions
    y0 = norm.pdf(x, mu0, se)
    y1 = norm.pdf(x, mu1, se)

    z_crit = norm.ppf(1 - alpha)
    critical_val = mu0 + z_crit * se

    plt.plot(x, y0, label='H0: μ=100')
    plt.plot(x, y1, label='H1: μ=105')
    plt.axvline(critical_val, color='red', linestyle='--', label='Critical Value')
    plt.fill_between(x, y0, where=(x > critical_val), color='red', alpha=0.3, label='Type I Error')
    plt.fill_between(x, y1, where=(x < critical_val), color='blue', alpha=0.3, label='Type II Error')
    plt.legend()
    plt.title("Type I and Type II Errors")
    plt.grid(True)
    plt.show()

visualize_type_errors()

#Write a Python program to perform an independent T-test and interpret the results
from scipy.stats import ttest_ind

group1 = np.random.normal(50, 5, 30)
group2 = np.random.normal(52, 5, 30)

t_stat, p = ttest_ind(group1, group2)
print("T-statistic:", t_stat, "| P-value:", p)

#Perform a paired sample T-test using Python and visualize the comparison results
from scipy.stats import ttest_rel

before = np.random.normal(70, 10, 20)
after = before + np.random.normal(2, 5, 20)

t_stat, p_val = ttest_rel(before, after)
print("Paired T-test:", t_stat, "| P-value:", p_val)

plt.plot(before, label='Before')
plt.plot(after, label='After')
plt.title("Before vs After (Paired T-test)")
plt.legend()
plt.grid(True)
plt.show()

#Simulate data and perform both Z-test and T-test, then compare the results using Python
from scipy.stats import ttest_1samp

data = np.random.normal(101, 10, 25)
pop_mean = 100
pop_std = 10

z = (np.mean(data) - pop_mean) / (pop_std / np.sqrt(len(data)))
_, t_p = ttest_1samp(data, pop_mean)
z_p = 2 * (1 - norm.cdf(abs(z)))

print(f"Z-test P-value: {z_p:.4f}, T-test P-value: {t_p:.4f}")

#Write a Python function to calculate the confidence interval for a sample mean and explain its significance
def confidence_interval(data, confidence=0.95):
    mean = np.mean(data)
    sem = stats.sem(data)
    interval = stats.t.interval(confidence, len(data)-1, loc=mean, scale=sem)
    print(f"{int(confidence*100)}% CI: {interval}")
    return interval

sample = np.random.normal(100, 15, 30)
confidence_interval(sample)

#Write a Python program to calculate the margin of error for a given confidence level using sample data
def margin_of_error(data, confidence=0.95):
    sem = stats.sem(data)
    moe = stats.t.ppf((1 + confidence) / 2, len(data) - 1) * sem
    print(f"Margin of Error: {moe:.2f}")
    return moe

margin_of_error(sample)

#C Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process
def bayes_theorem(prior_A, prob_B_given_A, prob_B_given_not_A):
    prior_not_A = 1 - prior_A
    prob_B = prob_B_given_A * prior_A + prob_B_given_not_A * prior_not_A
    posterior_A = (prob_B_given_A * prior_A) / prob_B
    return posterior_A

posterior = bayes_theorem(0.01, 0.9, 0.05)
print(f"Posterior Probability (Bayes): {posterior:.4f}")

#Perform a Chi-square test for independence between two categorical variables in Python
import pandas as pd
from scipy.stats import chi2_contingency

data = [[10, 20], [20, 40]]  # Example contingency table
stat, p, dof, expected = chi2_contingency(data)
print("Chi-square stat:", stat)
print("P-value:", p)

#Write a Python program to calculate the expected frequencies for a Chi-square test based on observed
#data
observed = np.array([[10, 20], [20, 40]])
_, _, _, expected = chi2_contingency(observed)
print("Expected Frequencies:\n", expected)

# Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution
from scipy.stats import chisquare

observed = np.array([25, 30, 45])
expected = np.array([33.3, 33.3, 33.3])

chi_stat, p_val = chisquare(f_obs=observed, f_exp=expected)
print("Chi-square Stat:", chi_stat, "| P-value:", p_val)

#Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2

df = 5
x = np.linspace(0, 30, 1000)
y = chi2.pdf(x, df)

plt.plot(x, y, label=f'Chi-square (df={df})')
plt.title("Chi-square Distribution")
plt.xlabel("Value")
plt.ylabel("Probability Density")
plt.grid(True)
plt.legend()
plt.show()

#+ Implement an F-test using Python to compare the variances of two random samples
from scipy.stats import f

sample1 = np.random.normal(50, 5, 30)
sample2 = np.random.normal(60, 10, 30)

f_stat = np.var(sample1, ddof=1) / np.var(sample2, ddof=1)
df1, df2 = len(sample1)-1, len(sample2)-1
p = 1 - f.cdf(f_stat, df1, df2)

print("F-statistic:", f_stat)
print("P-value:", p)

#Write a Python program to perform an ANOVA test to compare means between multiple groups and
#interpret the results
from scipy.stats import f_oneway

group1 = np.random.normal(60, 5, 20)
group2 = np.random.normal(65, 5, 20)
group3 = np.random.normal(70, 5, 20)

f_stat, p = f_oneway(group1, group2, group3)
print("ANOVA F-statistic:", f_stat)
print("P-value:", p)

#Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results
import seaborn as sns
import pandas as pd

data = pd.DataFrame({
    'value': np.concatenate([group1, group2, group3]),
    'group': ['A']*20 + ['B']*20 + ['C']*20
})

sns.boxplot(x='group', y='value', data=data)
plt.title("One-way ANOVA Group Comparison")
plt.grid(True)
plt.show()

#Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA
from scipy.stats import shapiro, levene

def check_anova_assumptions(groups):
    print("Normality test:")
    for i, group in enumerate(groups):
        stat, p = shapiro(group)
        print(f"Group {i+1}: p={p:.4f} {'(normal)' if p > 0.05 else '(not normal)'}")

    stat, p = levene(*groups)
    print("\nEqual Variance (Levene’s Test): p =", p, "→", "Equal Variance" if p > 0.05 else "Unequal Variance")

check_anova_assumptions([group1, group2, group3])

# Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the
#results
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Simulated dataset
df = pd.DataFrame({
    'Score': np.random.normal(70, 10, 60),
    'Gender': ['Male']*30 + ['Female']*30,
    'Program': ['A']*15 + ['B']*15 + ['A']*15 + ['B']*15
})

model = ols('Score ~ C(Gender) + C(Program) + C(Gender):C(Program)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

sns.boxplot(x='Program', y='Score', hue='Gender', data=df)
plt.title("Two-Way ANOVA Interaction")
plt.grid(True)
plt.show()

#Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing
from scipy.stats import f

x = np.linspace(0, 5, 500)
df1, df2 = 5, 10
y = f.pdf(x, df1, df2)

plt.plot(x, y, label=f'F-distribution (df1={df1}, df2={df2})')
plt.title("F-distribution")
plt.xlabel("F Value")
plt.ylabel("Probability Density")
plt.legend()
plt.grid(True)
plt.show()

#Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group mean
data = np.random.normal(100, 10, 30)
mean = np.mean(data)
z = (mean - 100) / (10 / np.sqrt(len(data)))
p = 2 * (1 - norm.cdf(abs(z)))

print(f"Z = {z:.2f}, P = {p:.4f}")

#Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means
observed_var = np.var(data, ddof=1)
hypothesized_var = 100
n = len(data)
chi2_stat = (n - 1) * observed_var / hypothesized_var
p_value = 2 * min(chi2.cdf(chi2_stat, n-1), 1 - chi2.cdf(chi2_stat, n-1))

print("Chi-square Stat:", chi2_stat, "| P-value:", p_value)

#Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results
from statsmodels.stats.proportion import proportions_ztest

count = np.array([50, 60])
nobs = np.array([100, 120])
stat, pval = proportions_ztest(count, nobs)

print("Z-statistic:", stat, "| P-value:", pval)

#Write a Python script to perform a Z-test for comparing proportions between two datasets or groups
f_val = np.var(group1, ddof=1) / np.var(group2, ddof=1)
x = np.linspace(0.1, 5, 500)
y = f.pdf(x, len(group1)-1, len(group2)-1)

plt.plot(x, y, label="F-distribution")
plt.axvline(f_val, color='red', linestyle='--', label=f"F = {f_val:.2f}")
plt.title("F-test for Variance")
plt.legend()
plt.grid(True)
plt.show()

#Perform a Chi-square test for goodness of fit with simulated data and analyze the results
observed = np.random.randint(15, 30, size=4)
expected = np.array([25, 25, 25, 25])
chi_stat, p = chisquare(observed, expected)

print("Observed:", observed)
print("Expected:", expected)
print("Chi-square Stat:", chi_stat, "| P-value:", p)